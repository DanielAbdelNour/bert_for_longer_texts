{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b684d1c-49d8-4a89-80ee-8f54c4f6fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mbrzozowski/projects/media_monitoring/roberta_for_longer_texts\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ..\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e401bb0-7239-4f5f-8d60-641db84ea040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../resources/roberta were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../resources/roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train accuracy: 0.5875, Train loss: 0.6660354651510716\n",
      "Epoch: 0, Test accuracy: 0.65, Test loss: 0.5904278859496117\n",
      "Epoch: 1, Train accuracy: 0.78125, Train loss: 0.5293301593512296\n",
      "Epoch: 1, Test accuracy: 0.925, Test loss: 0.35317784547805786\n",
      "Epoch: 2, Train accuracy: 0.88125, Train loss: 0.34443826507776976\n",
      "Epoch: 2, Test accuracy: 0.95, Test loss: 0.1830226019024849\n",
      "Epoch: 3, Train accuracy: 0.9375, Train loss: 0.20902621131390334\n",
      "Epoch: 3, Test accuracy: 0.9375, Test loss: 0.17358638979494573\n",
      "Epoch: 4, Train accuracy: 0.96875, Train loss: 0.12159209074452519\n",
      "Epoch: 4, Test accuracy: 0.95, Test loss: 0.12857716977596284\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from config import VISIBLE_GPUS\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= VISIBLE_GPUS\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.roberta_main import RobertaClassificationModel\n",
    "\n",
    "SAMPLE_DATA_PATH = 'test/sample_data/sample_data.csv'\n",
    "\n",
    "# Loading data for tests\n",
    "df = pd.read_csv(SAMPLE_DATA_PATH)\n",
    "\n",
    "texts = df['sentence'].tolist() # list of texts\n",
    "labels = df['target'].tolist() # list of 0/1 labels\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loading model\n",
    "model = RobertaClassificationModel()\n",
    "# Fitting a model to training data for 5 epochs\n",
    "result = model.train_and_evaluate(X_train, X_test, y_train, y_test,epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48efa996-87a0-4fd3-9252-f49942c82f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c7c13f-6a9f-441e-818e-b6d8566e4131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|    |   train_acc |   train_loss |   test_acc |   test_loss |\\n|---:|------------:|-------------:|-----------:|------------:|\\n|  0 |     0.5875  |     0.666035 |     0.65   |    0.590428 |\\n|  1 |     0.78125 |     0.52933  |     0.925  |    0.353178 |\\n|  2 |     0.88125 |     0.344438 |     0.95   |    0.183023 |\\n|  3 |     0.9375  |     0.209026 |     0.9375 |    0.173586 |\\n|  4 |     0.96875 |     0.121592 |     0.95   |    0.128577 |'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27feb650-97c4-46e0-84ff-61f2635dcf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570a06b-9db0-4f85-8f90-956eacdfb3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136c07d-379e-40d4-870b-e2516a32cdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b698b3-6db6-4610-bcca-490c3914eaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-roberta_for_longer_texts]",
   "language": "python",
   "name": "conda-env-.conda-roberta_for_longer_texts-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
